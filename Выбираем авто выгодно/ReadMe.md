# Выбираем авто вгодно
- Автор: Сорокин Андрей Борисович 
- Kaggle nick - Andrei Sorokin
- Группа - DSPR-30
- email: andrew.b.sorokin@gmail.com
- Score: 14.54634
- Место в LB: 61

## Цель проекта - прогнозирование стоимости автомобиля по характеристикам.
## Файлы
- grabber.ipynb - модуль парсинга страниц с auto.ru
- sf-dst-car-price-prediction-by-sand.ipynb - модуль решения

## План решения
- Анализ тестового набора для определения параметров выборки с сайита auto.ru
- По полученным критериям получить список линков на объявления о продаже авто
- По списку, используя wget, скачать страницы на локальный диск
- Парсинг страниц - получение тренировочного датасета
- EDA
- Подготовка данных к работе (очистка, кодирование, масштабирование)
- Построение наивной модели
- Получение первых результатов и метрики локально и на kaggle 
- 9-N Feature engeneering и model selection

## Признаки
- brand - бренд производителя
- color - цвет машины
- mileage - пробег
- model_date - год началапроизводства модели
- model_name - название модели
- number_of_doors - кол-во дверей
- production_date - год выпуска машины
- owners - кол-во владельцев
- pts_original - наличие оригинала ПТС
- steering_wheel - прворульная/леворульная
- body_type  - тип кузова
- displacement - объем двигателя
- engine_type - вид топлива
- gear_type - привод
- transmission - коробка
- power - мощность двигателя
- acceleration - разгон до 100 км
- clearance_min - минимальный дорожный просвет
- fuel_rate - расход топлива
- description  - описание от продавца
- equipment_dict  - список доп. оборудования
- sell_id  - идентификатор продавца
- price  - цена                                                            

## Рассмотренные модели
- RandomForestRegressor
- XGBRegressor
- ExtraTreesRegressor
- CatBoostRegressor
- LGBMRegressor
- Bagging Lasso, Ridge, ExtraTreeRegressor
- Stacking (RandomForestRegressor, XGBRegressor, CatBoostRegressor, LGBMRegressor) -> ExtraTreesRegressor
- Stacking (CatBoostRegressor, LGBMRegressor) -> XGBRegressor

## Выводы
Рассмотрены несколько моделей регрессии. Лучшие результаты показали CatBoost и LGBM. При использовании stacking можно добиться лучших результятов на трайне (CatBoost, LGBM, XGB, RandomForest) и ExtraTrees. 
Но похоже, что кроме накапливания лучших качеств моделей он также собирает и хучшие, например склонность к переобучению. Поэтому на тестовых данных в LB он показал себя хуже.
Что удалось: попробовать много моделей регресии, разобраться с TF-IDF и попробовать кластеризацию K-Means на нем, попробовать разные методы поиска оптимальных гиперпараметров.
Не удалось: выйти в топ LB :(, попробовти другие модели обработки текстов (WordToVec), попробовать другие методы кластеризации (уж больно они долго работают - боялся время зря потерять).
На качество модели сильно влияет то, что данные теста собраны на 2020 год, а данныые трейна получилось собрать только на 2012 год. 
Чтобы с этим справиться скорретировал результат на средний процент прироста цены по моделям, взяв данные за 2020 год из базового решения.
